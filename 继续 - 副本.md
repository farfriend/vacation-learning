#### 决策树（Quinlan）

基本思路：通过一些数据集（x中的属性）训练出整套决策算法。（需要用到递归思想）

![https://s4.ax1x.com/2022/01/28/7zCjQ1.png](https://s4.ax1x.com/2022/01/28/7zCjQ1.png)

​	

​	如何划分每个节点：

​	  重要公式：

​		注：假定当前样本集合D中第k类样本所占的比例为
$$
p_k(k=1,2,…|y|)
$$

##### 			信息熵（information entropy）：

$$
Ent（D)=-\sum\limits_{k=1}^{|y|}p_klog_2p_k
$$

​			Ent(D)的值越小，则D的纯度越高

​			定义：离散属性a有V个可能的取值{a^1,a^2,…a^V},用a对样本集D划分会有V个分支结点，其中第V个分支结点包含了D所有再属性a上取值为a^v的样本，记为D^v.

##### 			信息增益（information gain）：(对可取值数目较多的属性偏好)

​			
$$
Gain(D,a)=Ent(D)-\sum\limits_{v=1}^V|D^{v}|/|D|Ent(D^{v})
$$
​			一般来说，信息增益越大，意味着属性a来进行划分所获得的“纯度提升”越大。（讲普通点，谁信息增益越大，谁被选为划分属性，即第一个结点）

##### 		增益率（gain ratio）：（对可取值数目较少的属性有所偏好）

​		

​	
$$
Gain_ratio(D,a)=Gain(D,a)/IV(a)
$$
​			其中，
$$
IV(a)=-\sum\limits_{v=1}^V|D^v|/|D|log_2(|D^v|/|D|)
$$

##### 		基尼指数（Gini index)（可用于选择最优划分属性）

$$
Gini(D)=\sum_{k=1}^{|y|}\sum_{k^{'}}p_kp_{k'}
=1-\sum_{k=1}^{|y|}p_k^2
$$

​		Gini(D)越小，则数据集D的纯度越高。剪枝处理（处理”过拟合“的问题）

![https://s4.ax1x.com/2022/01/28/7zP9oD.png](https://s4.ax1x.com/2022/01/28/7zP9oD.png)

##### 	预剪枝（对划分前后的泛化性能进行估计）

<u>将样本分为验证集和训练集两种。根据信息增益选择属性作为“根”，再一步一步通过验证划分前后验证集精度来决定最终是否划分，得到最终的决策树。</u>

##### 集精度（p）：

$$
P=分类正确的样例数/总样例数
$$



![https://s4.ax1x.com/2022/01/28/7zPQYQ.png](https://s4.ax1x.com/2022/01/28/7zPQYQ.png)

![https://s4.ax1x.com/2022/01/28/7zPNwT.png](https://s4.ax1x.com/2022/01/28/7zPNwT.png)

**注：预剪枝决策树会带来欠拟合的风险**

##### 后剪枝（解决欠拟合问题）

后剪枝先从训练集生成一颗完整决策树，从最后的分支开始，假设将分支划分为叶结点，通过计算假设前后的精度，决定是否划分。

##### 连续值处理

对于连续值属性离散化技术排不上用场，最简单的策略是采用二分法。

![https://s4.ax1x.com/2022/01/28/7zPwY4.png](https://s4.ax1x.com/2022/01/28/7zPwY4.png)
$$
T_a=\lbrace{\frac{a^i+a^{i+1}}{2}}|1\leq{i}\leq{n-1}\rbrace
$$
即把区间平均值作为候选划分点。

此时我们就可以像离散值一样来考察这些划分点，并选取最优的。

利用信息增益公式，稍加改造：
$$
Gain(D,a)=\max\limits_{t\in{T_a}}Gain(D,a,t)
=\max\limits_{t\in{T_a}}Ent(D)-\sum\limits_{\lambda\in\lbrace-,+\rbrace}\frac{|D^\lambda_t|}{|D|}Ent(D^{\lambda}_t)
$$
其中，Gain(D,a,t)是样本集D基于划分点t二分后的信息增益。

例子：

![https://s4.ax1x.com/2022/01/28/7zPDp9.png](https://s4.ax1x.com/2022/01/28/7zPDp9.png)

![https://s4.ax1x.com/2022/01/28/7ziPBV.png](https://s4.ax1x.com/2022/01/28/7ziPBV.png)

##### 缺失值处理

问题一：如何在属性值缺失的情况下进行划分属性选择

问题二：给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分。

![https://s4.ax1x.com/2022/01/28/7ziQHK.png](https://s4.ax1x.com/2022/01/28/7ziQHK.png)

![https://s4.ax1x.com/2022/01/28/7ziY3d.png](https://s4.ax1x.com/2022/01/28/7ziY3d.png)

据上定义，我们可将信息增益的计算式推广：
$$
Gain(D,a)=ρ*Gain(\tilde{D},a)=ρ*(Ent(\tilde{D})-\sum_{v=1}^VEnt(\tilde{D}^v))
$$

$$
Ent(\tilde{D})=-\sum_{k=1}^{|y|}\tilde{p}_klog_2\tilde{p}_k
$$

##### 多变量决策树

